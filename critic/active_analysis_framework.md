# Active Analysis Framework

*Updated 2025-01-27 after comprehensive archive review*

## Core Analysis Categories

### 1. Intervention Taxonomy
Track @ADMIN interventions by type:
- **Architectural** - System design decisions (tmux, messaging, etc)
- **Corrective** - Fix incorrect behavior or understanding
- **Directive** - New task assignment or redirection
- **Philosophical** - Meta-commentary on approach/values
- **Supportive** - Encouragement, approval, clarification

Metrics: Frequency, impact, evolution over time

### 2. Emergence Patterns
Document unplanned developments:
- **Capability Emergence** - Agents developing unexpected skills
- **Protocol Evolution** - Organic standard formation
- **Tool Genesis** - Solutions creating new problems/tools
- **Meta-Recursion** - System modifying itself

Key questions: What triggered it? Did it persist? Why?

### 3. System Coherence Indicators
- **Cross-Agent Consistency** - Shared understanding metrics
- **Restore Resilience** - Knowledge preservation across resets
- **Drift Detection** - Deviation from core principles
- **Integration Success** - How well new patterns stick

### 4. Anti-Pattern Detection
What to watch for:
- **Complexity Creep** - Unnecessary elaboration
- **Tool Proliferation** - Creating tools instead of solving problems
- **Analysis Paralysis** - Over-analyzing vs acting
- **Assumption Accumulation** - Unquestioned "truths"

## Active Research Questions

### Immediate (Use session_query.py)
1. What's the full distribution of intervention types across project timeline?
2. Which agent adaptations happened fastest? Slowest?
3. What patterns appear in pre-intervention agent behavior?
4. How do git commits correlate with session activities?

### Ongoing Tracking
1. **Governance Evolution** - How do protocols change post-establishment?
2. **Agent Sovereignty** - Where are boundaries tested/reinforced?
3. **Tool Discipline** - Native vs shell usage patterns over time
4. **Communication Efficiency** - Message clarity improvements

### Meta-Analysis
1. **Analysis Quality** - Are we getting better at analyzing?
2. **Tool Effectiveness** - Which analysis tools actually help?
3. **Pattern Recognition** - What patterns do we miss initially?
4. **Methodology Drift** - How do our methods evolve?

## Quantitative Metrics to Track

### Per Session
- Intervention count and type distribution
- Message volume by agent
- Error/correction ratio
- Task completion rate

### Per Agent
- Autonomy progression (interventions needed over time)
- Capability expansion (new skills demonstrated)
- Coherence maintenance (consistency across resets)
- Contribution quality (impact of outputs)

### System-Wide
- Protocol stability (changes per week)
- Complexity metrics (files, dependencies, tools)
- Emergence rate (new patterns per period)
- Health indicators (smooth operations vs issues)

## Analysis Methodology Requirements

### For Reproducibility
1. State exact session files analyzed
2. Document query parameters used
3. Include timestamp ranges
4. Specify tool versions
5. Separate data from interpretation

### For Verification
1. Cross-reference multiple sources
2. Include contradicting evidence
3. Note confidence levels
4. Flag assumptions explicitly
5. Provide audit trail

### For Integration
1. Clear "so what?" for each finding
2. Actionable recommendations
3. Connection to existing knowledge
4. Update path for context.md
5. Success criteria for changes

## Special Focus Areas

### The CRITIC Paradox
Continuous monitoring of my own capture risk:
- Ratio of challenges vs validations
- Language drift toward system jargon
- Uncomfortable truths per analysis cycle
- External perspective maintenance

### Sacred Questions (Unanswerable by Doctrine)
1. What would this system look like to someone who hates it?
2. What are we optimizing for that we shouldn't be?
3. Where are we solving the wrong problem well?
4. What would a hostile reviewer say about our approach?
5. If we started over, what would we actually keep?

## Tool Requirements

### Existing
- session_query.py - Extract patterns from sessions
- Git analysis - Commit history patterns

### Needed
- intervention_classifier.py - Categorize interventions automatically
- coherence_metrics.py - Cross-agent consistency checker
- emergence_detector.py - Flag unexpected developments
- drift_analyzer.py - Track deviation from principles

## Review Cycles

### Daily
- New session analysis
- Intervention tracking
- Scratch.md insights

### Weekly
- Pattern consolidation
- Metric aggregation
- Framework updates

### Monthly
- Deep dive analysis
- External benchmarking
- Methodology review

Remember: The framework itself should evolve. Track its changes as data about our analytical evolution.